{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b539a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4, re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "from bs4 import Comment\n",
    "import time\n",
    "import random\n",
    "from fake_useragent import UserAgent\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624074d9",
   "metadata": {},
   "source": [
    "## Get the data via web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e1bf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1 of 32 teams\n",
      "Completed 2 of 32 teams\n",
      "Completed 3 of 32 teams\n",
      "Completed 4 of 32 teams\n",
      "Completed 5 of 32 teams\n",
      "Completed 6 of 32 teams\n",
      "Completed 7 of 32 teams\n",
      "Completed 8 of 32 teams\n",
      "Completed 9 of 32 teams\n",
      "Completed 10 of 32 teams\n",
      "Completed 11 of 32 teams\n",
      "Completed 12 of 32 teams\n",
      "Completed 13 of 32 teams\n",
      "Completed 14 of 32 teams\n",
      "Completed 15 of 32 teams\n",
      "Completed 16 of 32 teams\n",
      "Completed 17 of 32 teams\n",
      "Completed 18 of 32 teams\n",
      "Completed 19 of 32 teams\n",
      "Completed 20 of 32 teams\n",
      "Completed 21 of 32 teams\n",
      "Completed 22 of 32 teams\n",
      "Completed 23 of 32 teams\n",
      "Completed 24 of 32 teams\n",
      "Completed 25 of 32 teams\n",
      "Completed 26 of 32 teams\n",
      "Completed 27 of 32 teams\n",
      "Completed 28 of 32 teams\n",
      "Completed 29 of 32 teams\n",
      "Completed 30 of 32 teams\n",
      "Completed 31 of 32 teams\n",
      "Completed 32 of 32 teams\n"
     ]
    }
   ],
   "source": [
    "teams = [\"buf\", \"mia\", \"nwe\", \"nyj\", \"pit\", \"rav\", \"cle\", \"cin\", \"oti\", \"clt\", \"htx\", \"jax\", \"kan\", \"rai\", \n",
    "         \"den\", \"sdg\", \"was\", \"nyg\", \"dal\", \"phi\", \"gnb\", \"chi\", \"min\", \"det\", \"nor\", \"tam\", \"atl\", \"car\", \n",
    "         \"sea\", \"ram\", \"crd\", \"sfo\"]\n",
    "\n",
    "data_accum = []\n",
    "\n",
    "# Use random user agent when scraping.\n",
    "ua = UserAgent()\n",
    "user_agent = {'User-agent': ua.random}\n",
    "\n",
    "for t in teams:\n",
    "    url = f\"https://www.pro-football-reference.com/teams/{t}/2020_roster.htm\"\n",
    "    \n",
    "    response = requests.get(url, headers = user_agent)\n",
    "    status = response.status_code\n",
    "    if status == 200:\n",
    "        page = response.text\n",
    "        soup = bs(page, \"html.parser\")\n",
    "    else:\n",
    "        print(f\"Oops! Received status code {status}\")\n",
    "        \n",
    "    # The table is in a comment so parse it again with BeautifulSoup.\n",
    "    temp_table = soup.find(attrs = {\"class\":\"table_wrapper\", \"id\":\"all_roster\"})\n",
    "    comment = temp_table.find(text=lambda text: isinstance(text, Comment))\n",
    "    commentSoup = bs(comment, \"html.parser\")\n",
    "\n",
    "    # Now sort through the web page.\n",
    "    rows = commentSoup.find_all(\"tr\")\n",
    "\n",
    "    # Get column names.\n",
    "    column_headers = [x.text for x in rows[0].find_all(\"th\")]\n",
    "    column_headers.extend([\"Team\", \"Record\"])\n",
    "\n",
    "    # Get player stats.\n",
    "    players = []\n",
    "    for e in rows[1:]:\n",
    "        # Table has uniform number as \"th\".\n",
    "        temp = [e.find(\"th\").text] + [x.text for x in e.find_all(\"td\")]\n",
    "        temp.append(t)\n",
    "\n",
    "        # From original page get the team's record.\n",
    "        record = soup.find(text=re.compile(\"Record\")).next.split(\",\")[0].strip()\n",
    "        temp.append(record)\n",
    "        players.append(temp)\n",
    "        \n",
    "    # Convert collected data to dictionary and add to list accumulator.\n",
    "    for e in players:\n",
    "        dct = dict()\n",
    "        for h, p in zip(column_headers, e):\n",
    "            dct[h] = p\n",
    "        data_accum.append(dct)\n",
    "    \n",
    "    # Add (1.5-15sec) delay that makes the web scraping more human-like.\n",
    "    timeout = 1.5*random.randint(1,10)\n",
    "    print(\"Completed\", teams.index(t)+1, \"of\", len(teams), \"teams\")\n",
    "    time.sleep(timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2a5bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to dataframe and save.\n",
    "\n",
    "df = pd.DataFrame(data_accum)\n",
    "\n",
    "with open('./data/web_scraped_dataframe.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4565023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
